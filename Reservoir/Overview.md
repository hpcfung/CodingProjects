Here we have a simple neural network (NN) with 6 layers (4 hidden layers), trained to classify a simplified version of MNIST.
Conceptually, the NN can be decomposed into 3 parts: the features (the first hidden layer), the "reservoir" 
(the 2-4th hidden layers), and "regression" (the final neuron).

### Features

### Reservoir

### Regression

## Results

When overfitting, most of the learning occurs in the reservoir?

Changing the algorithm structure, towards "rote memorization"?

Counterargument: this is just fine-tuning?


